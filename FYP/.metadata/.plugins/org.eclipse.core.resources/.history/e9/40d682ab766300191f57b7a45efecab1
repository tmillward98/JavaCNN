package cnn.layers;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.stream.DoubleStream;

import org.xml.sax.InputSource;

import cnn.layers.neurons.*;

public class FCC extends Layer {
	
	protected Layer nextLayer;
	protected Layer previousLayer;
	private double[][] mapFlat;
	private double[] flatInputs;
	//private ArrayList<Neuron> neurons;
	
	public void forwardPropagate(){
		output = new ArrayList<double[][]>();
		flattenInputs();
		
		double sum = 0;
		
		for(int i = 0; i < neurons.size(); i++) {
			neurons.get(i).receiveInput(flatInputs);
			mapFlat[i][0] = neurons.get(i).forward();
			sum+= mapFlat[i][0];
		}
		
		for(int i = 0; i < mapFlat.length; i++) {
			mapFlat[i][0] = mapFlat[i][0] / sum;
		}
		
		output.add(mapFlat);
	}
	
	//At this layer, we already receive the deltas for updating the weights
	public void backwardPropagate(ArrayList<Double> prevDelta, double lr) {
		
		//As this is the output layer, the error is already calculated. 
		//As such, delta is calculated as error * transfer derivative (one delta per neuron)
		
		//System.out.println(prevDelta.get(0));
		
		//We receive the error for this layer (expected - actual)
		deltas = prevDelta;
		
		//And so our delta for this layer is the error * derivative
		for(int i = 0; i < neurons.size(); i++) {
			deltas.set(i, deltas.get(i) * neurons.get(i).getDerivative());
		}
		
		
		//Before updating the weights, we calculate the deltas for the previous layer
		//This is equal to the sum of (weights in a neuron * their delta)

		
		//For each weight in the current layer (i)
		//For each neuron in the current layer (i)
		//Access weight[i] in neuron [j]
		//error = sum of all weight[i] in each neuron * the given neurons delta (j)
		
		//ALL THE WEIGHTS IN THE NEURON ARE THE WEIGHTS THAT CONNECT CURRENT NEURON TO THE PREVIOUS NEURON
		
		ArrayList<Double> errors = new ArrayList<Double>();
				
		
		//For each neuron in this layer (current neuron)
		//Weight k connects each neuron in previous layer with current neuron in this layer
		
		for(int i = 0; i < neurons.size(); i++) {
			double error = 0;
			for(int k = 0; k < neurons.get(0).getWeights().length; k++) {
				error += neurons.get(i).getWeight(k) * deltas.get(i);
			}
			errors.add(error);
		}
		
		
		
		for(int i = 0; i < neurons.get(0).getWeights().length; i++) {
			double error = 0;
			for(int j = 0; j < neurons.size(); j++) {
				error += neurons.get(j).getWeight(i) * deltas.get(j);
			}
			errors.add(error);
			error = 0;
		}
		
		
		for(int i = 0; i < neurons.size(); i++) {
			neurons.get(i).updateWeights(deltas.get(i), lr);
		}
		
		previousLayer.backwardPropagate(errors, lr);
		
	}

	public ArrayList<double[][]> initialiseLayer(int c, ArrayList<double[][]> exampleInput, Layer nl, Layer pl) {
		nextLayer = nl;
		previousLayer = pl;
		neurons = new ArrayList<Neuron>();
		
		input = exampleInput;
		
		//Create a random number of neurons
		createNeurons(c, input.get(0).length);
		
		return input;
	}
	
	private void createNeurons(int n, int w) {
		for (int i = 0; i < n; i++) {
			ClassNeuron a = new ClassNeuron("Class " + i, w);
			neurons.add(a);
		}
	}
	
	private void flattenInputs() {	
		ArrayList<double[]> flatInput = new ArrayList<double[]>();
		for(int i = 0; i < input.size(); i++) {
			flatInput.add(Arrays.stream(input.get(i))
			        .flatMapToDouble(Arrays::stream)
			        .toArray());
		}
		flatInputs = flatInput.get(0);
		for(int i = 1; i < flatInput.size(); i++) {
			flatInputs = DoubleStream.concat(Arrays.stream(flatInputs), Arrays.stream(flatInput.get(i))).toArray();
		}
		
		mapFlat = new double[neurons.size()][1];
		
	}

	public int getCount() {
		return neurons.size();
	}
}

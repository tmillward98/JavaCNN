package cnn.layers;

import java.util.ArrayList;
import java.util.Arrays;
import cnn.layers.neurons.ClassNeuron;
import cnn.layers.neurons.FCNeuron;
import cnn.layers.neurons.InputNeuron;

import java.util.stream.*;

/**
 * Layer which contains all information on the fully connected structure
 * Makes use of arbitrary functions previous provided
 * Ultimately produces a classification vector
 * @author Tom
 *
 */
public class FCLayer extends Layer {

	Layer previousLayer;
	Layer nextLayer;
	ArrayList<InputNeuron> layerNeurons;
	ArrayList<ClassNeuron> classNeurons;
	ArrayList<double[][]> input;
	double[] flatInputs;
	ArrayList<double[][]> output;
	
	double sc = 0;
	double[] temp;
	double[] results;
	
	/**
	 * FC LAYER
	 * FINAL LAYER SHOULD BE EQUAL TO NUMBER OF CLASSES WE WISH TO IDENTIFY (EG. EACH NEURON OUTPUTS PROBABILITY)
	 * P(C|X)
	 * 
	 * FIRST LAYER TAKES INPUTS (NUMBER OF FEATURE MAPS)
	 * APPLY VECTOR OF WEIGHTS (EG. EACH NEURON HAS A VECTOR OF WEIGHTS = SIZE OF INPUT)
	 * WEIGHTS ARE INITIALISED AS RANDOM
	 * 
	 * EVERY OUTPUT OF EVERY NEURON IS THEN PASSED TO EVERY NEURON IN NEXT LAYER
	 * WEIGHTS/BIAS APPLIED/ACTIVATION FUNCTIONS APPLIED
	 * 
	 * FINAL LAYER CLASSIFIES IMAGE BASED ON INPUTS (FEATURE MAPS THROUGH NEURONS)
	 * THIS PRODUCES A PROBABILITY
	 * 
	 * OUTPUT SHOULD BE A C x 1 VECTOR WITH NORMALISED PROBABILITIES FROM ALL NEURONS (ALL CLASSES)
	 * 
	 * ERROR DETERMINED BY SUM OF ERROR FROM VECTORS
	 * BACKPROPOGATE, ADJUST WEIGHTS
	 * 
	 * eg. output produces vector
	 * Element wise value multiplication e^ci / sum of C values
	 * 
	 * Fully connected denotes the output of every neuron should go to every neuron in the next layer
	 * 
	 */
	
	public FCLayer() {
		layerNeurons = new ArrayList<InputNeuron>();
		classNeurons = new ArrayList<ClassNeuron>();
	}
	
	public ArrayList<double[][]> initaliseLayer(int c, double[][] sampleImage){
		//Create neurons based on how many inputs
		input = new ArrayList<double[][]>();
		input.add(sampleImage);
		createInputNeurons();
		return input;
	}
	
	/**
	 * Retrieve scores from each class neuron, normalise probabilities in the range of 0-1
	 */
	private void softmax() {	
		for(int i = 0; i < results.length; i++) {
			sc += classNeurons.get(i).forward();
			results[i] = Math.exp(classNeurons.get(i).forward());
		}
		for(double n : results) 
			n = n / sc;
	}
	
	/**
	 * Number of neurons equal to the flattened volume of one instance of input
	 */
	private void createInputNeurons() {
		
		temp = Arrays.stream(input.get(0))
		        .flatMapToDouble(Arrays::stream)
		        .toArray();
		
		for (int i = 0; i < temp.length; i++) {
			FCNeuron a = new FCNeuron();
			layerNeurons.add(a);
		}
	}
	
	public void assignLayer(Layer prev, Layer nl) {
		previousLayer = prev; nextLayer = nl;
	}
	
	public ArrayList<double[][]> forwardPropagate(){
		input = previousLayer.forwardPropagate();
		flattenInputs();
		//do neuron stuff, return vector of outputs
		//begin back propagation based on error of result
		return input;
	}
	
	private void flattenInputs() {	
		ArrayList<double[]> flatInput = new ArrayList<double[]>();
		for(int i = 0; i < input.size(); i++) {
			flatInput.add(Arrays.stream(input.get(i))
			        .flatMapToDouble(Arrays::stream)
			        .toArray());
		}
		flatInputs = flatInput.get(0);
		for(int i = 1; i < flatInput.size(); i++) {
			flatInputs = DoubleStream.concat(Arrays.stream(flatInputs), Arrays.stream(flatInput.get(i))).toArray();
		}
		
	}
	
}

